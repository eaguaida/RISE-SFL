{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "rise_sfl_path = \"/content/RISE-SFL/RISE-SFL\"  # Replace with the actual path to your RISE-SFL folder\n",
    "sys.path.append(rise_sfl_path)\n",
    "from utils.utils import *\n",
    "# From masker/generation.py\n",
    "from masker.generation import SFL\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "from torch.nn.functional import softmax, conv2d\n",
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "# Load black box model for explanations\n",
    "model = models.resnet50(True)\n",
    "model = nn.Sequential(model, nn.Softmax(dim=1))\n",
    "model = model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/19 [00:00<?, ?it/s]c:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\RISE-SFL\\RISE-SFL\\masker\\generation.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cell_size = torch.ceil(torch.tensor(self.input_size).float() / s)\n",
      "Processing images: 100%|██████████| 19/19 [02:52<00:00,  9.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from masker.batch import SFL_batch\n",
    "\n",
    "input_size = (224,224)\n",
    "sfl_batch = SFL_batch(model, input_size)\n",
    "image_folder = r'C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\RISE-SFL\\test_data'\n",
    "N = 100\n",
    "s = 8\n",
    "p1 = 0.5\n",
    "masks, sampled_tensors, target_list = sfl_batch.generate_batch_images(image_folder, N, s, p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pass_fail_list(N):\n",
    "    return [\"Pass\" if i % 2 == 0 else \"Fail\" for i in range(N)]\n",
    "\n",
    "pass_fail_list = generate_pass_fail_list(N)\n",
    "\n",
    "from visuals.plots import SFLVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d942f4dab2e94d7181a00eb8f19edbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Mutant Index:', max=99), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#If we want to observe any of the images that are being explained, we just need to \n",
    "visualizer = SFLVisualizer()\n",
    "visualizer.interactive_mutant_visualization(sampled_tensors[0], pass_fail_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfl_techniques.sfl import RelevanceScore\n",
    "#A dictionary will be return for the image to be explained\n",
    "\n",
    "# For each image, after returning the dictionary, we need to use causal metrics to calculate which one is more efficient\n",
    "relevance_score_calculator = RelevanceScore(device='cuda')\n",
    "\n",
    "pixel_datasets = []\n",
    "ochiai_array = []\n",
    "zoltar_array = []\n",
    "tarantula_array = []\n",
    "wong1_array = []\n",
    "for i in range(19):\n",
    "    pixel_dataset, ochiai, tarantula, zoltar, wong1 = relevance_score_calculator.run(sampled_tensors[i], masks[i], N)\n",
    "    pixel_datasets.append(pixel_dataset)\n",
    "    ochiai_array.append(ochiai)\n",
    "    zoltar_array.append(zoltar)\n",
    "    tarantula_array.append(tarantula)\n",
    "    wong1_array.append(wong1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimage_path = os.path.join(image_folder, image_files[0])\\n\\nsaliency_map = SaliencyMapVisualizer(image_path)\\nsaliency_map.visualize_pixel_scores(pixel_datasets[0],ins='')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from visuals.saliency_maps.saliency import SaliencyMapVisualizer\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(image_folder)\n",
    "\n",
    "# Filter for .jpg and .png files\n",
    "image_files = [file for file in all_files if file.lower().endswith(('.jpg', '.png'))]\n",
    "'''\n",
    "image_path = os.path.join(image_folder, image_files[0])\n",
    "\n",
    "saliency_map = SaliencyMapVisualizer(image_path)\n",
    "saliency_map.visualize_pixel_scores(pixel_datasets[0],ins='')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rise_sfl_path = r\"C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\metrics-saliency-maps\"  # Replace with the actual path to your RISE-SFL folder\n",
    "sys.path.append(rise_sfl_path)\n",
    "#saliency_maps = extract_specific_saliency_maps(pixel_datasets[0])\n",
    "\n",
    "from saliency_maps_metrics.multi_step_metrics import Deletion, Insertion, compute_correlation, compute_auc_metric\n",
    "from saliency_maps_metrics.data_replace import select_data_replace_method\n",
    "\n",
    "deletion_metric = Deletion(data_replace_method=\"black\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "insertion_metric = Insertion(data_replace_method=\"blur\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn = 2\\nimage_path = os.path.join(image_folder, image_files[n])\\nimg = read_tensor(image_path).to(device)\\n# Create instances of Deletion and Insertion metrics\\nmodel.to(device)\\ntarget_class = target_list[n]\\n# Initialize lists to store results\\n\\n# Function to calculate sparsity\\ndef calculate_sparsity(saliency_map):\\n    max_value = np.max(saliency_map)\\n    mean_value = np.mean(saliency_map)\\n    return max_value / mean_value if mean_value != 0 else float(\\'inf\\')\\n\\n# Create a dictionary of saliency maps\\nsaliency_maps = {\\n    \"Wong1\": wong1_array[n][0],\\n    \"Tarantula\": tarantula_array[n][0],\\n    \"Zoltar\": zoltar_array[n][0],\\n    \"Ochiai\": ochiai_array[n][0]\\n}\\n\\ndevice=\\'cuda\\'\\n# Create instances of Deletion and Insertion metrics\\ndeletion_metric = Deletion(data_replace_method=\"black\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\\ninsertion_metric = Insertion(data_replace_method=\"blur\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\\n\\n# Initialize lists to store results\\nresults = []\\n\\n# Loop through each saliency map\\nfor name, saliency_map in saliency_maps.items():\\n    # Convert saliency map to PyTorch tensor and add batch and channel dimensions\\n    saliency_map_tensor = torch.from_numpy(saliency_map).float().unsqueeze(0).unsqueeze(0)\\n    saliency_map_tensor = saliency_map_tensor.to(device)\\n\\n    # Compute Deletion metrics\\n    deletion_result = deletion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\\n    dauc_mean = deletion_result[\"dauc\"]\\n    dc_mean = deletion_result[\"dc\"]\\n\\n    # Compute Insertion metrics\\n    insertion_result = insertion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\\n    iauc_mean = insertion_result[\"iauc\"]\\n    ic_mean = insertion_result[\"ic\"]\\n    \\n    # Calculate sparsity\\n    sparsity = calculate_sparsity(saliency_map)\\n    # Store results\\n    results.append([name, dauc_mean, dc_mean, iauc_mean, ic_mean, sparsity])\\n\\n# Create a table using tabulate\\nheaders = [\"Saliency Map\", \"DAUC\", \"DC\", \"IAUC\", \"IC\",\"Sparsity\"]\\ntable = tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\")\\n\\n# Print the table\\nprint(f\"Metrics for target class: {target_class}\")\\nprint(table)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tabulate import tabulate  \n",
    "device='cuda'\n",
    "'''\n",
    "n = 2\n",
    "image_path = os.path.join(image_folder, image_files[n])\n",
    "img = read_tensor(image_path).to(device)\n",
    "# Create instances of Deletion and Insertion metrics\n",
    "model.to(device)\n",
    "target_class = target_list[n]\n",
    "# Initialize lists to store results\n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(saliency_map):\n",
    "    max_value = np.max(saliency_map)\n",
    "    mean_value = np.mean(saliency_map)\n",
    "    return max_value / mean_value if mean_value != 0 else float('inf')\n",
    "\n",
    "# Create a dictionary of saliency maps\n",
    "saliency_maps = {\n",
    "    \"Wong1\": wong1_array[n][0],\n",
    "    \"Tarantula\": tarantula_array[n][0],\n",
    "    \"Zoltar\": zoltar_array[n][0],\n",
    "    \"Ochiai\": ochiai_array[n][0]\n",
    "}\n",
    "\n",
    "device='cuda'\n",
    "# Create instances of Deletion and Insertion metrics\n",
    "deletion_metric = Deletion(data_replace_method=\"black\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "insertion_metric = Insertion(data_replace_method=\"blur\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Loop through each saliency map\n",
    "for name, saliency_map in saliency_maps.items():\n",
    "    # Convert saliency map to PyTorch tensor and add batch and channel dimensions\n",
    "    saliency_map_tensor = torch.from_numpy(saliency_map).float().unsqueeze(0).unsqueeze(0)\n",
    "    saliency_map_tensor = saliency_map_tensor.to(device)\n",
    "\n",
    "    # Compute Deletion metrics\n",
    "    deletion_result = deletion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "    dauc_mean = deletion_result[\"dauc\"]\n",
    "    dc_mean = deletion_result[\"dc\"]\n",
    "\n",
    "    # Compute Insertion metrics\n",
    "    insertion_result = insertion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "    iauc_mean = insertion_result[\"iauc\"]\n",
    "    ic_mean = insertion_result[\"ic\"]\n",
    "    \n",
    "    # Calculate sparsity\n",
    "    sparsity = calculate_sparsity(saliency_map)\n",
    "    # Store results\n",
    "    results.append([name, dauc_mean, dc_mean, iauc_mean, ic_mean, sparsity])\n",
    "\n",
    "# Create a table using tabulate\n",
    "headers = [\"Saliency Map\", \"DAUC\", \"DC\", \"IAUC\", \"IC\",\"Sparsity\"]\n",
    "table = tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\")\n",
    "\n",
    "# Print the table\n",
    "print(f\"Metrics for target class: {target_class}\")\n",
    "print(table)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 image files.\n",
      "Average Metrics for 19 images:\n",
      "+----------------+------------+----------+------------+----------+----------------+\n",
      "| Saliency Map   |   Avg DAUC |   Avg DC |   Avg IAUC |   Avg IC |   Avg Sparsity |\n",
      "+================+============+==========+============+==========+================+\n",
      "| Wong1          |     0.1322 |   0.2064 |     0.7327 |   0.1107 |         2.5028 |\n",
      "+----------------+------------+----------+------------+----------+----------------+\n",
      "| Tarantula      |     0.1332 |   0.2148 |     0.7391 |   0.1256 |         2.8643 |\n",
      "+----------------+------------+----------+------------+----------+----------------+\n",
      "| Zoltar         |     0.1293 |   0.1633 |     0.7245 |   0.1131 |         1.5394 |\n",
      "+----------------+------------+----------+------------+----------+----------------+\n",
      "| Ochiai         |     0.1294 |   0.2091 |     0.7256 |   0.1361 |         2.7933 |\n",
      "+----------------+------------+----------+------------+----------+----------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def get_all_image_files(directory):\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    return image_files\n",
    "\n",
    "# Use the function to get all image files\n",
    "image_files = get_all_image_files(image_folder)\n",
    "\n",
    "# Check if there are any image files in the directory\n",
    "if not image_files:\n",
    "    print(\"No image files found in the specified directory or its subdirectories.\")\n",
    "else:\n",
    "    print(f\"Found {len(image_files)} image files.\")\n",
    "\n",
    "    device = 'cuda'\n",
    "    model.to(device)\n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(saliency_map):\n",
    "    max_value = np.max(saliency_map)\n",
    "    mean_value = np.mean(saliency_map)\n",
    "    return max_value / mean_value if mean_value != 0 else float('inf')\n",
    "\n",
    "# Create instances of Deletion and Insertion metrics\n",
    "deletion_metric = Deletion(data_replace_method=\"black\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "insertion_metric = Insertion(data_replace_method=\"blur\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "\n",
    "# Initialize dictionary to store cumulative results\n",
    "cumulative_results = {\n",
    "    \"Wong1\": {\"DAUC\": 0, \"DC\": 0, \"IAUC\": 0, \"IC\": 0, \"Sparsity\": 0},\n",
    "    \"Tarantula\": {\"DAUC\": 0, \"DC\": 0, \"IAUC\": 0, \"IC\": 0, \"Sparsity\": 0},\n",
    "    \"Zoltar\": {\"DAUC\": 0, \"DC\": 0, \"IAUC\": 0, \"IC\": 0, \"Sparsity\": 0},\n",
    "    \"Ochiai\": {\"DAUC\": 0, \"DC\": 0, \"IAUC\": 0, \"IC\": 0, \"Sparsity\": 0}\n",
    "}\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Process each image\n",
    "for i, (image_file, target_class) in enumerate(zip(image_files, target_list)):\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    img = read_tensor(image_path).to(device)\n",
    "\n",
    "    # Create a dictionary of saliency maps for this image\n",
    "    saliency_maps = {\n",
    "        \"Wong1\": wong1_array[i][0],\n",
    "        \"Tarantula\": tarantula_array[i][0],\n",
    "        \"Zoltar\": zoltar_array[i][0],\n",
    "        \"Ochiai\": ochiai_array[i][0]\n",
    "    }\n",
    "\n",
    "    # Process each saliency map for this image\n",
    "    for name, saliency_map in saliency_maps.items():\n",
    "        saliency_map_tensor = torch.from_numpy(saliency_map).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute Deletion metrics\n",
    "        deletion_result = deletion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "        cumulative_results[name][\"DAUC\"] += deletion_result[\"dauc\"]\n",
    "        cumulative_results[name][\"DC\"] += deletion_result[\"dc\"]\n",
    "\n",
    "        # Compute Insertion metrics\n",
    "        insertion_result = insertion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "        cumulative_results[name][\"IAUC\"] += insertion_result[\"iauc\"]\n",
    "        cumulative_results[name][\"IC\"] += insertion_result[\"ic\"]\n",
    "\n",
    "        # Calculate sparsity\n",
    "        cumulative_results[name][\"Sparsity\"] += calculate_sparsity(saliency_map)\n",
    "\n",
    "# Calculate averages\n",
    "num_images = len(image_files)\n",
    "results = []\n",
    "for name, metrics in cumulative_results.items():\n",
    "    avg_metrics = [name] + [metrics[key] / num_images for key in [\"DAUC\", \"DC\", \"IAUC\", \"IC\", \"Sparsity\"]]\n",
    "    results.append(avg_metrics)\n",
    "\n",
    "# Create a table using tabulate\n",
    "headers = [\"Saliency Map\", \"Avg DAUC\", \"Avg DC\", \"Avg IAUC\", \"Avg IC\", \"Avg Sparsity\"]\n",
    "table = tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\")\n",
    "\n",
    "# Print the table\n",
    "print(f\"Average Metrics for {num_images} images:\")\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
